---
title: "Guide Complet : Sobri√©t√© des Donn√©es"
date: 2025-01-27
draft: false
weight: 4
description: "Le manuel de r√©f√©rence pour une gestion responsable des donn√©es : de la collecte minimale √† la suppression active, en passant par la gouvernance et la conformit√© RGPD."
---

Les donn√©es sont le carburant de l'√©conomie num√©rique, mais leur accumulation incontr√¥l√©e repr√©sente un d√©fi environnemental majeur. Les data centers mondiaux consomment plus de 400 TWh d'√©lectricit√© par an, et le stockage en repr√©sente une part croissante. Ce guide complet vous accompagne dans la mise en place d'une gestion sobre et responsable des donn√©es.

**Temps de lecture estim√© : 45 minutes**

---

## 1. Le probl√®me des donn√©es

### 1.1 L'explosion volum√©trique

#### Croissance exponentielle

Le volume de donn√©es mondial conna√Æt une croissance sans pr√©c√©dent.

**√âvolution du datasphere mondial :**
| Ann√©e | Volume total | Croissance annuelle |
|-------|-------------|---------------------|
| 2010 | 2 ZB | - |
| 2015 | 15 ZB | +50%/an |
| 2020 | 64 ZB | +25%/an |
| 2024 | 147 ZB | +23%/an |
| 2028 (proj.) | 394 ZB | +28%/an |

*1 ZB (Zettabyte) = 1 milliard de Terabytes = 1 000 milliards de Gigabytes*

#### Sources de cette croissance

**Donn√©es g√©n√©r√©es par les utilisateurs :**
- Photos et vid√©os (smartphones, r√©seaux sociaux)
- Communications (emails, messageries)
- Documents collaboratifs
- R√©seaux sociaux

**Donn√©es machine-to-machine :**
- IoT et capteurs (milliards d'objets connect√©s)
- Logs et m√©triques applicatives
- T√©l√©m√©trie et monitoring
- Donn√©es de g√©olocalisation

**Donn√©es d'entreprise :**
- Transactions et √©v√©nements business
- Analytics et tracking
- Backups et r√©plications
- Archives r√©glementaires

**Intelligence artificielle :**
- Datasets d'entra√Ænement
- Mod√®les et poids (LLM : dizaines de Go par mod√®le)
- Donn√©es de fine-tuning
- Outputs et historiques

### 1.2 Le co√ªt environnemental du stockage

#### Impact √©nerg√©tique

Contrairement √† l'intuition, le stockage consomme de l'√©nergie en permanence, pas seulement lors des acc√®s.

**Consommation par type de stockage :**
| Type | Puissance par To | Consommation annuelle |
|------|-----------------|----------------------|
| HDD data center | 4-8 W | 35-70 kWh |
| SSD data center | 1-3 W | 9-26 kWh |
| Object storage (cloud) | 0.5-2 W* | 4-18 kWh |
| Tape (archive) | ~0 W (hors acc√®s) | <1 kWh |

*Inclut l'overhead d'infrastructure

**Empreinte carbone du stockage :**
```
Empreinte = Volume √ó Dur√©e √ó (Consommation + Overhead PUE) √ó Facteur carbone

Exemple :
- 100 To stock√©s pendant 1 an
- Cloud (r√©gion France, 60 gCO2e/kWh)
- PUE moyen de 1.3
- Consommation : 1 W/To

Empreinte = 100 √ó 8760h √ó 1W √ó 1.3 √ó 60 / 1000000
         = 68.3 kgCO2e/an

Soit ~0.7 kgCO2e/To/an pour du cloud en France
(5-20 kgCO2e/To/an dans des r√©gions plus carbon√©es)
```

#### Impact mat√©riel

**Empreinte de fabrication des disques :**
| Type | Empreinte fabrication | Dur√©e de vie | Amortissement |
|------|----------------------|--------------|---------------|
| HDD 10 To | 50-80 kgCO2e | 4-6 ans | 10-20 kgCO2e/an |
| SSD 4 To | 80-120 kgCO2e | 5-7 ans | 15-25 kgCO2e/an |

**Ressources n√©cessaires :**
- Terres rares pour les composants √©lectroniques
- M√©taux pr√©cieux pour les connecteurs
- Plastiques et aluminium pour les bo√Ætiers
- √ânergie consid√©rable pour la fabrication

#### Impact r√©seau

Les donn√©es ne restent pas statiques. Elles sont :
- R√©pliqu√©es (redondance, haute disponibilit√©)
- Sauvegard√©es (backups locaux et distants)
- Synchronis√©es (multi-sites, edge)
- Transf√©r√©es (acc√®s utilisateurs, APIs)

**Multiplication des copies :**
```
Donn√©e originale : 1 To

Apr√®s infrastructure standard :
‚îú‚îÄ Copie locale (RAID) : √ó1.5 √† √ó3
‚îú‚îÄ Backup quotidien (7 jours) : √ó7
‚îú‚îÄ Backup mensuel (12 mois) : √ó12
‚îú‚îÄ R√©plication multi-r√©gion : √ó2-3
‚îî‚îÄ Snapshots et versions : √ó1.5-3

Volume r√©el : 10 √† 50 To pour 1 To "utile"
```

### 1.3 Le dark data

#### D√©finition

Le "dark data" d√©signe les donn√©es collect√©es, trait√©es et stock√©es mais jamais utilis√©es ni analys√©es.

**√âtudes Gartner et Veritas :**
- 50-80% des donn√©es d'entreprise sont du dark data
- 33% des donn√©es n'ont jamais √©t√© acc√©d√©es depuis leur cr√©ation
- Moins de 10% des donn√©es stock√©es sont activement utilis√©es

#### Types de dark data

**Donn√©es oubli√©es :**
- Anciens projets jamais archiv√©s ou supprim√©s
- Comptes d'employ√©s partis non nettoy√©s
- Donn√©es de tests et d√©veloppement en production
- Exports et rapports ponctuels jamais supprim√©s

**Donn√©es dupliqu√©es :**
- Versions multiples du m√™me fichier
- Copies locales de documents partag√©s
- Emails transf√©r√©s avec pi√®ces jointes r√©p√©t√©es
- Backups redondants

**Donn√©es ROT (Redundant, Obsolete, Trivial) :**
- **Redundant** : doublons et copies inutiles
- **Obsolete** : donn√©es p√©rim√©es sans valeur
- **Trivial** : donn√©es sans importance business

**Logs et donn√©es techniques :**
- Logs de debug jamais analys√©s
- M√©triques √† grain fin non exploit√©es
- Historiques de requ√™tes
- Donn√©es de t√©l√©m√©trie brutes

#### Co√ªt du dark data

```
Calcul du co√ªt du dark data :

Hypoth√®ses entreprise moyenne :
- Stockage total : 500 To
- Dark data : 60% = 300 To
- Co√ªt stockage cloud : 20‚Ç¨/To/mois

Co√ªt annuel du dark data :
300 To √ó 20‚Ç¨ √ó 12 mois = 72 000‚Ç¨/an

Empreinte carbone (cloud France) :
300 To √ó 0.7 kgCO2e = 210 kgCO2e/an

Pour une grande entreprise (50 Po de donn√©es) :
30 Po de dark data √ó 20‚Ç¨ √ó 12 = 7.2 M‚Ç¨/an
```

### 1.4 R√©glementation et contraintes

#### RGPD - Principe de minimisation

L'article 5 du RGPD impose :
- **Minimisation** : collecter uniquement les donn√©es ad√©quates, pertinentes et limit√©es
- **Limitation de conservation** : dur√©es d√©finies et respect√©es
- **Exactitude** : donn√©es √† jour, inexactes effac√©es

#### Droit √† l'effacement (article 17)

Les personnes concern√©es peuvent demander l'effacement :
- Donn√©es plus n√©cessaires aux finalit√©s
- Consentement retir√©
- Opposition au traitement
- Traitement illicite

#### Obligations sectorielles

| Secteur | R√©glementation | Dur√©es typiques |
|---------|---------------|-----------------|
| Finance | B√¢le III, MiFID II | 5-10 ans |
| Sant√© | RGPD sant√©, HDS | Variable, jusqu'√† 20 ans |
| Commerce | Code de commerce | 10 ans (factures) |
| RH | Code du travail | 5 ans apr√®s d√©part |
| Secteur public | Archives publiques | 5-100 ans selon nature |

---

## 2. Principes de sobri√©t√©

### 2.1 Le paradigme de la sobri√©t√©

#### Inverser la logique par d√©faut

**Approche traditionnelle :**
```
Collecter tout ‚Üí Stocker ind√©finiment ‚Üí Peut-√™tre utiliser un jour
```

**Approche sobre :**
```
D√©finir le besoin ‚Üí Collecter le minimum ‚Üí Dur√©e de vie explicite ‚Üí Suppression automatique
```

#### Les quatre questions fondamentales

Avant toute collecte ou conservation, se poser :

1. **Pourquoi ?** Quelle est la finalit√© pr√©cise de cette donn√©e ?
2. **Combien ?** Quelle quantit√© est r√©ellement n√©cessaire ?
3. **Combien de temps ?** Quelle dur√©e de conservation justifi√©e ?
4. **Qui ?** Qui est responsable de sa gestion et suppression ?

### 2.2 Collecte minimale

#### Principe "Privacy by Design"

Int√©grer la minimisation d√®s la conception :
- Identifier les donn√©es strictement n√©cessaires
- Documenter la justification de chaque champ
- Pr√©voir la dur√©e de vie d√®s la collecte
- Designer pour la suppression

#### Exemples de minimisation

**Formulaires web :**
```
‚ùå Formulaire maximaliste :
- Nom, Pr√©nom, Email, T√©l√©phone, Adresse compl√®te
- Date de naissance, Sexe, Situation familiale
- Profession, Entreprise, Revenus
- "Comment nous avez-vous connu ?"
- Centres d'int√©r√™t (checkboxes multiples)

‚úÖ Formulaire sobre :
- Email (obligatoire, pour contact)
- Pr√©nom (optionnel, pour personnalisation)
- Message (essentiel)
```

**Analytics :**
```
‚ùå Tracking maximaliste :
- User ID permanent
- Toutes les pages vues
- Temps pass√© par pixel
- Mouvements de souris
- Device fingerprint complet

‚úÖ Analytics sobre :
- Pages vues agr√©g√©es (pas de user ID)
- M√©triques essentielles uniquement
- Agr√©gation imm√©diate (pas de donn√©es brutes)
- Anonymisation ou pseudonymisation
```

**Logs applicatifs :**
```
‚ùå Logging excessif :
log.debug(f"User {user_id} requested {full_request_body}")
log.debug(f"Response: {full_response_body}")
log.debug(f"Processing time for each step: {detailed_timings}")

‚úÖ Logging sobre :
log.info(f"Request: {request_id} {method} {path} -> {status_code} {duration_ms}ms")
# D√©tails uniquement si erreur
if status_code >= 400:
    log.error(f"Error details: {error_type} - {error_message}")
```

### 2.3 Dur√©es de r√©tention explicites

#### Matrice de r√©tention type

| Cat√©gorie | Type de donn√©es | Dur√©e | Justification |
|-----------|----------------|-------|---------------|
| **Op√©rationnel** | Sessions utilisateur | 24h-7j | S√©curit√©, debug |
| | Cache applicatif | Minutes-heures | Performance |
| | Files d'attente | Jusqu'√† traitement | Workflow |
| **Technique** | Logs applicatifs (info) | 30-90 jours | Debug, monitoring |
| | Logs s√©curit√© | 1-2 ans | Conformit√©, audit |
| | M√©triques agr√©g√©es | 2-5 ans | Tendances |
| | M√©triques d√©taill√©es | 7-30 jours | Investigation |
| **Business** | Transactions | 10 ans | L√©gal (commerce) |
| | Contrats | Dur√©e + 5-10 ans | Prescription |
| | Factures | 10 ans | Code commerce |
| **Personnel** | Donn√©es employ√©s | Dur√©e contrat + 5 ans | Obligations l√©gales |
| | Donn√©es candidats non retenus | 2 ans max | RGPD + recrutement |
| | Donn√©es clients | Dur√©e relation + 3 ans | Prescription commerciale |
| **Archives** | Documents historiques | D√©finitif (selon valeur) | M√©moire d'entreprise |
| | Projets termin√©s | 3-10 ans | R√©f√©rence, litiges |

#### Impl√©mentation technique

**Configuration TTL (Time To Live) :**

```yaml
# Redis - TTL automatique
SET session:user123 "data" EX 86400  # Expire en 24h

# Elasticsearch - Index lifecycle
PUT _ilm/policy/logs_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_age": "7d",
            "max_size": "50gb"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": { "number_of_shards": 1 }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "freeze": {}
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

```sql
-- PostgreSQL - Partitionnement avec r√©tention
CREATE TABLE events (
    id BIGSERIAL,
    created_at TIMESTAMP NOT NULL,
    data JSONB
) PARTITION BY RANGE (created_at);

-- Cr√©er des partitions mensuelles
CREATE TABLE events_2024_01 PARTITION OF events
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

-- Script de purge des anciennes partitions
DO $$
DECLARE
    partition_name TEXT;
    cutoff_date DATE := CURRENT_DATE - INTERVAL '90 days';
BEGIN
    FOR partition_name IN
        SELECT tablename FROM pg_tables
        WHERE tablename LIKE 'events_%'
        AND substring(tablename from 'events_(\d{4}_\d{2})')::DATE < cutoff_date
    LOOP
        EXECUTE format('DROP TABLE %I', partition_name);
        RAISE NOTICE 'Dropped partition: %', partition_name;
    END LOOP;
END $$;
```

### 2.4 Suppression active

#### Automatisation des purges

**Principes :**
- La suppression doit √™tre automatique, pas manuelle
- Jobs de purge planifi√©s et monitor√©s
- Logs de suppression pour audit
- Alertes si les purges √©chouent

**Exemple de job de purge :**
```python
# Job de purge quotidien
import logging
from datetime import datetime, timedelta
from apscheduler.schedulers.blocking import BlockingScheduler

logger = logging.getLogger(__name__)

def purge_old_sessions():
    """Supprime les sessions de plus de 7 jours"""
    cutoff = datetime.utcnow() - timedelta(days=7)
    deleted = db.sessions.delete_many({"last_activity": {"$lt": cutoff}})
    logger.info(f"Purged {deleted.deleted_count} old sessions")
    metrics.record("sessions_purged", deleted.deleted_count)

def purge_old_logs():
    """Archive et supprime les logs de plus de 90 jours"""
    cutoff = datetime.utcnow() - timedelta(days=90)

    # Archiver d'abord
    logs_to_archive = db.logs.find({"timestamp": {"$lt": cutoff}})
    archive_to_cold_storage(logs_to_archive)

    # Puis supprimer
    deleted = db.logs.delete_many({"timestamp": {"$lt": cutoff}})
    logger.info(f"Purged {deleted.deleted_count} old logs")

def purge_orphaned_files():
    """Supprime les fichiers non r√©f√©renc√©s"""
    referenced_ids = set(db.documents.distinct("file_id"))
    all_files = storage.list_files("uploads/")

    orphans = [f for f in all_files if f.id not in referenced_ids]
    for orphan in orphans:
        storage.delete(orphan)

    logger.info(f"Purged {len(orphans)} orphaned files")

scheduler = BlockingScheduler()
scheduler.add_job(purge_old_sessions, 'cron', hour=2)
scheduler.add_job(purge_old_logs, 'cron', hour=3)
scheduler.add_job(purge_orphaned_files, 'cron', day_of_week='sun', hour=4)
scheduler.start()
```

#### Nettoyage assist√© utilisateur

**Interface de gestion du stockage :**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Mon espace de stockage - 45 Go utilis√©s           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  R√©partition :                                              ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Documents : 23 Go          ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Emails : 12 Go             ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Photos : 7 Go              ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Autres : 3 Go              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚ö†Ô∏è Suggestions de nettoyage :                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ üìÅ Fichiers volumineux non ouverts depuis 1 an          ‚îÇ‚îÇ
‚îÇ  ‚îÇ    12 fichiers - 8.5 Go                    [Voir]       ‚îÇ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÇ
‚îÇ  ‚îÇ üìß Emails avec pi√®ces jointes > 5 Mo                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ    234 emails - 4.2 Go                     [Voir]       ‚îÇ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§‚îÇ
‚îÇ  ‚îÇ üìã Doublons d√©tect√©s                                    ‚îÇ‚îÇ
‚îÇ  ‚îÇ    45 fichiers - 2.1 Go                    [Voir]       ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  √âconomie potentielle : 14.8 Go                             ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.5 √âviter les doublons

#### Sources de duplication

**Comportements utilisateurs :**
- Copier un fichier partag√© en local "au cas o√π"
- Cr√©er des versions manuelles ("rapport_v2_final_vraifinal.docx")
- Garder les pi√®ces jointes des emails + les stocker ailleurs

**Architecture syst√®me :**
- Synchronisation multi-devices sans d√©duplication
- Backups incr√©mentaux mal configur√©s
- R√©plication excessive pour la "s√©curit√©"

#### Strat√©gies de d√©duplication

**Niveau utilisateur :**
```
Politique de nommage :
‚îú‚îÄ Un seul emplacement de r√©f√©rence par document
‚îú‚îÄ Liens plut√¥t que copies
‚îú‚îÄ Versioning automatique (pas de _v2, _final)
‚îî‚îÄ Nettoyage r√©gulier des espaces personnels
```

**Niveau syst√®me :**
```python
# D√©duplication par hash au stockage
import hashlib

def store_file(content, filename):
    # Calculer le hash du contenu
    file_hash = hashlib.sha256(content).hexdigest()

    # V√©rifier si le contenu existe d√©j√†
    existing = storage.get_by_hash(file_hash)
    if existing:
        # Cr√©er seulement une r√©f√©rence
        return create_reference(filename, existing.id)

    # Sinon, stocker le nouveau contenu
    return storage.store(content, file_hash, filename)
```

**Niveau infrastructure :**
- D√©duplication au niveau stockage (NetApp, ZFS, etc.)
- Content-addressable storage
- Single-instance storage pour les emails

---

## 3. Gouvernance des donn√©es

### 3.1 Cartographie et inventaire

#### Pourquoi cartographier ?

Impossible de g√©rer ce qu'on ne conna√Æt pas. La cartographie permet de :
- Identifier les volumes et leur r√©partition
- Rep√©rer le dark data et les doublons
- D√©finir les responsabilit√©s
- Prioriser les actions de nettoyage

#### M√©thodologie de cartographie

**√âtape 1 : Inventaire des syst√®mes de stockage**
```
Inventaire technique :
‚îú‚îÄ Bases de donn√©es
‚îÇ   ‚îú‚îÄ PostgreSQL production : 2.3 To
‚îÇ   ‚îú‚îÄ MongoDB analytics : 5.1 To
‚îÇ   ‚îî‚îÄ Redis cache : 50 Go
‚îú‚îÄ Stockage fichiers
‚îÇ   ‚îú‚îÄ S3 documents : 12 To
‚îÇ   ‚îú‚îÄ NAS interne : 8 To
‚îÇ   ‚îî‚îÄ OneDrive utilisateurs : 15 To
‚îú‚îÄ Emails
‚îÇ   ‚îî‚îÄ Exchange Online : 4 To
‚îú‚îÄ Applications SaaS
‚îÇ   ‚îú‚îÄ Salesforce : 500 Go
‚îÇ   ‚îî‚îÄ Slack : 200 Go
‚îî‚îÄ Backups
    ‚îú‚îÄ Veeam : 45 To
    ‚îî‚îÄ AWS S3 Glacier : 30 To

TOTAL : ~120 To
```

**√âtape 2 : Classification des donn√©es**
```yaml
# Sch√©ma de classification
categories:
  - name: "Donn√©es personnelles"
    sensitivity: high
    retention: "Dur√©e relation + 3 ans"
    examples:
      - "Donn√©es clients CRM"
      - "Donn√©es employ√©s RH"

  - name: "Donn√©es business"
    sensitivity: medium
    retention: "10 ans (l√©gal)"
    examples:
      - "Factures et contrats"
      - "Transactions"

  - name: "Donn√©es techniques"
    sensitivity: low
    retention: "30-90 jours"
    examples:
      - "Logs applicatifs"
      - "M√©triques"

  - name: "Donn√©es projet"
    sensitivity: medium
    retention: "Dur√©e projet + 3 ans"
    examples:
      - "Documents de travail"
      - "Livrables"
```

**√âtape 3 : Analyse d'usage**
```sql
-- Analyse d'acc√®s aux fichiers (exemple)
SELECT
    date_trunc('month', last_accessed) as month,
    count(*) as file_count,
    sum(size_bytes) / 1024 / 1024 / 1024 as size_gb
FROM files
GROUP BY 1
ORDER BY 1;

-- R√©sultat exemple :
-- month      | file_count | size_gb
-- -----------+------------+---------
-- 2024-12    | 15,234     | 45.2    (acc√®s r√©cent)
-- 2024-06    | 8,432      | 23.1    (6 mois)
-- 2023-12    | 12,876     | 67.3    (>1 an = dark data potentiel)
-- Jamais     | 34,521     | 89.4    (dark data certain)
```

### 3.2 Politiques de r√©tention

#### Structure d'une politique

```markdown
# Politique de r√©tention des donn√©es

## 1. Objet et p√©rim√®tre
Cette politique d√©finit les dur√©es de conservation des donn√©es
de [Entreprise] et les proc√©dures de suppression associ√©es.

## 2. Principes g√©n√©raux
- Toute donn√©e collect√©e doit avoir une dur√©e de vie d√©finie
- La suppression est automatique sauf exception document√©e
- Les d√©rogations requi√®rent une validation Data Owner + DPO

## 3. Dur√©es de r√©tention par cat√©gorie

### 3.1 Donn√©es clients
| Sous-cat√©gorie | Dur√©e | Base l√©gale |
|----------------|-------|-------------|
| Donn√©es de contact | Relation + 3 ans | Prescription commerciale |
| Historique achats | 10 ans | Code commerce |
| Donn√©es de navigation | 13 mois | CNIL |

### 3.2 Donn√©es employ√©s
| Sous-cat√©gorie | Dur√©e | Base l√©gale |
|----------------|-------|-------------|
| Dossier personnel | D√©part + 5 ans | Prescription sociale |
| Bulletins de paie | D√©part + 5 ans | Code du travail |
| Documents m√©dicaux | D√©part + 40 ans | Code de la sant√© |

### 3.3 Donn√©es techniques
| Sous-cat√©gorie | Dur√©e | Justification |
|----------------|-------|---------------|
| Logs applicatifs | 90 jours | Debug et investigation |
| Logs s√©curit√© | 2 ans | Conformit√© ISO 27001 |
| M√©triques | 2 ans agr√©g√©es, 30j d√©taill√©es | Capacity planning |

## 4. Proc√©dures de suppression

### 4.1 Suppression automatique
- Jobs de purge quotidiens pour les donn√©es techniques
- Archivage puis suppression pour les donn√©es business
- V√©rification et audit mensuel des processus

### 4.2 Suppression sur demande
- Traitement des demandes RGPD sous 30 jours
- Validation par le Data Owner avant ex√©cution
- Certificat de suppression fourni

## 5. Exceptions et d√©rogations
Les d√©rogations doivent √™tre :
- Document√©es avec justification
- Limit√©es dans le temps
- Valid√©es par Data Owner et DPO
- R√©vis√©es annuellement

## 6. Responsabilit√©s
- **Data Owners** : d√©finition des dur√©es m√©tier
- **IT** : impl√©mentation technique des purges
- **DPO** : validation conformit√© RGPD
- **Audit** : contr√¥le annuel

## 7. R√©vision
Cette politique est r√©vis√©e annuellement ou lors de tout
changement r√©glementaire significatif.
```

### 3.3 R√¥les et responsabilit√©s

#### RACI des donn√©es

| Activit√© | Data Owner | IT | DPO | Utilisateur |
|----------|-----------|----|----|-------------|
| D√©finir les dur√©es de r√©tention | A/R | C | C | I |
| Impl√©menter les purges | C | R | I | - |
| Valider les exceptions | A | I | C | R |
| Nettoyer les espaces personnels | I | C | - | R |
| R√©pondre aux demandes d'effacement | C | R | A | - |
| Auditer la conformit√© | I | C | A/R | - |

*R: Responsible, A: Accountable, C: Consulted, I: Informed*

#### Profils de Data Owners

**Par domaine m√©tier :**
- **DRH** : donn√©es employ√©s, candidats
- **Direction commerciale** : donn√©es clients, prospects
- **Direction financi√®re** : donn√©es comptables, factures
- **DSI** : donn√©es techniques, logs, m√©triques

**Responsabilit√©s du Data Owner :**
1. D√©finir les finalit√©s et usages l√©gitimes
2. Valider les dur√©es de r√©tention
3. Approuver les acc√®s et partages
4. D√©cider des suppressions exceptionnelles
5. R√©pondre aux demandes d'acc√®s/rectification

### 3.4 Processus de revue

#### Revue p√©riodique

**Revue mensuelle (automatis√©e) :**
```
Rapport automatique mensuel :
‚îú‚îÄ Volume total et √©volution (+/-%)
‚îú‚îÄ Alertes de d√©passement de quotas
‚îú‚îÄ Purges effectu√©es (succ√®s/√©checs)
‚îú‚îÄ Donn√©es approchant fin de r√©tention
‚îî‚îÄ Anomalies d√©tect√©es
```

**Revue trimestrielle (Data Owner) :**
- Analyse du dark data identifi√©
- Validation des suppressions manuelles
- R√©vision des exceptions en cours
- Priorisation des actions de nettoyage

**Revue annuelle (Comit√©) :**
- Bilan volum√©trique et tendances
- R√©vision de la politique de r√©tention
- Mise √† jour des dur√©es l√©gales
- Objectifs de r√©duction pour l'ann√©e

---

## 4. Donn√©es personnelles et RGPD

### 4.1 Principes de minimisation

#### Article 5.1.c du RGPD

> "Les donn√©es √† caract√®re personnel doivent √™tre ad√©quates, pertinentes et limit√©es √† ce qui est n√©cessaire au regard des finalit√©s pour lesquelles elles sont trait√©es (minimisation des donn√©es)."

#### Application pratique

**Test de n√©cessit√© pour chaque donn√©e :**
```
Pour chaque champ collect√©, r√©pondre √† :

1. Cette donn√©e est-elle N√âCESSAIRE pour la finalit√© d√©clar√©e ?
   (pas "utile" ou "pratique" - N√âCESSAIRE)

2. Existe-t-il un moyen d'atteindre l'objectif avec MOINS de donn√©es ?
   (agr√©gation, pseudonymisation, √©chantillonnage)

3. La granularit√© est-elle JUSTIFI√âE ?
   (date de naissance compl√®te vs tranche d'√¢ge)

Si la r√©ponse est "non" √† 1 ou "oui" √† 2-3 ‚Üí ne pas collecter
```

**Exemples d'exc√®s courants :**

| Finalit√© | Donn√©es excessives | Donn√©es suffisantes |
|----------|-------------------|---------------------|
| Newsletter | Nom, pr√©nom, email, t√©l√©phone, adresse | Email |
| Devis | Date naissance, CSP, revenus | Email, besoin exprim√© |
| Satisfaction | Nom, email, historique complet | Notation anonyme |
| Analytics | User ID, parcours d√©taill√© | Pages vues agr√©g√©es |

### 4.2 Dur√©es l√©gales de conservation

#### R√©f√©rentiel CNIL

| Finalit√© | Dur√©e maximale | Source |
|----------|---------------|--------|
| Prospection commerciale | 3 ans apr√®s dernier contact | CNIL |
| Donn√©es de navigation web | 13 mois | CNIL (cookies) |
| Vid√©osurveillance | 1 mois | Code de la s√©curit√© |
| Donn√©es de connexion (FAI) | 1 an | LCEN |
| Donn√©es RH (contrat actif) | Dur√©e du contrat | CNIL |
| Donn√©es RH (apr√®s d√©part) | 5 ans | Prescription sociale |
| Donn√©es de sant√© | 20 ans min | Code de la sant√© |
| Contentieux | Jusqu'√† √©puisement voies de recours | - |

#### Points de d√©part des dur√©es

Attention au calcul du point de d√©part :
- **Derni√®re interaction** : pour la prospection
- **Fin de la relation** : pour les clients
- **D√©part de l'entreprise** : pour les salari√©s
- **Fin du litige** : pour les donn√©es contentieuses

### 4.3 Droits des personnes

#### Droit √† l'effacement (article 17)

**Conditions d'application :**
- Donn√©es plus n√©cessaires aux finalit√©s
- Retrait du consentement
- Opposition au traitement
- Traitement illicite
- Obligation l√©gale d'effacement

**Exceptions (l'effacement peut √™tre refus√© si) :**
- Libert√© d'expression et d'information
- Obligation l√©gale de conservation
- Motifs d'int√©r√™t public (sant√©, archives)
- Constatation, exercice ou d√©fense de droits en justice

**Proc√©dure type :**
```
R√©ception demande (J)
    ‚Üì
V√©rification identit√© (J √† J+3)
    ‚Üì
Analyse du p√©rim√®tre (J+3 √† J+10)
‚îú‚îÄ Quelles donn√©es ?
‚îú‚îÄ Quels syst√®mes ?
‚îú‚îÄ Exceptions applicables ?
    ‚Üì
Ex√©cution de l'effacement (J+10 √† J+25)
‚îú‚îÄ Bases de production
‚îú‚îÄ Backups (si techniquement possible)
‚îú‚îÄ Archives
‚îú‚îÄ Sous-traitants
    ‚Üì
Confirmation √† la personne (J+30 max)
‚îú‚îÄ Liste des donn√©es effac√©es
‚îú‚îÄ Donn√©es conserv√©es (avec justification)
```

### 4.4 Anonymisation et pseudonymisation

#### Diff√©rences

| Aspect | Pseudonymisation | Anonymisation |
|--------|-----------------|---------------|
| R√©versibilit√© | Oui (avec la cl√©) | Non (irr√©versible) |
| Statut RGPD | Donn√©es personnelles | Hors scope RGPD |
| Techniques | Hashage, tokenisation, chiffrement | Agr√©gation, g√©n√©ralisation, bruit |
| Utilit√© | S√©curit√©, s√©paration des droits | Analytics, open data |

#### Techniques d'anonymisation

**G√©n√©ralisation :**
```
Avant : Jean Dupont, 35 ans, Paris 15e
Apr√®s : H, 30-40 ans, Paris
```

**Agr√©gation :**
```
Avant : Achats individuels par client
Apr√®s : Moyenne des achats par tranche d'√¢ge et r√©gion
```

**Perturbation (ajout de bruit) :**
```python
# K-anonymit√© par ajout de bruit
import numpy as np

def add_noise(value, epsilon=0.1):
    noise = np.random.laplace(0, 1/epsilon)
    return value + noise
```

**Suppression :**
```
Avant : {nom: "Dupont", email: "j.dupont@mail.com", age: 35}
Apr√®s : {age: 35}  # Identifiants directs supprim√©s
```

#### V√©rification de l'anonymisation

**Crit√®res du Groupe de travail Article 29 :**
1. **Individualisation** : impossible d'isoler un individu
2. **Corr√©lation** : impossible de relier des enregistrements
3. **Inf√©rence** : impossible de d√©duire des informations

```python
# Test simple de k-anonymit√©
def check_k_anonymity(df, quasi_identifiers, k=5):
    """V√©rifie que chaque combinaison appara√Æt au moins k fois"""
    counts = df.groupby(quasi_identifiers).size()
    return counts.min() >= k

# Usage
quasi_ids = ['age_group', 'region', 'gender']
is_k_anonymous = check_k_anonymity(anonymized_df, quasi_ids, k=5)
```

---

## 5. Donn√©es applicatives

### 5.1 Logs et monitoring

#### Niveaux de log appropri√©s

| Environnement | Niveau | R√©tention |
|---------------|--------|-----------|
| D√©veloppement | DEBUG | Session |
| Test/QA | DEBUG | 7 jours |
| Staging | INFO | 14 jours |
| Production | WARN/ERROR + INFO s√©lectif | 30-90 jours |

**Configuration par environnement :**
```yaml
# logging.yaml
production:
  level: INFO
  filters:
    - exclude_debug_modules
  handlers:
    - rotating_file:
        max_size: 100MB
        backup_count: 30
    - elasticsearch:
        index_pattern: "logs-prod-{date}"
        retention_days: 90

development:
  level: DEBUG
  handlers:
    - console
    - file:
        path: /tmp/app.log
```

#### Structuration des logs

**Logs structur√©s (JSON) vs texte brut :**
```python
# ‚ùå Log non structur√©
logging.info(f"User {user_id} logged in from {ip} at {timestamp}")

# ‚úÖ Log structur√©
logging.info("user_login", extra={
    "user_id": user_id,
    "ip": ip,
    "timestamp": timestamp,
    "event_type": "authentication"
})
```

**Avantages des logs structur√©s :**
- Analyse plus facile (requ√™tes, filtres)
- Compression plus efficace
- Agr√©gation automatisable
- Anonymisation cibl√©e possible

#### R√©duction du volume de logs

```python
# √âchantillonnage pour logs √† haut volume
import random

class SampledLogger:
    def __init__(self, logger, sample_rate=0.1):
        self.logger = logger
        self.sample_rate = sample_rate

    def info(self, msg, **kwargs):
        if random.random() < self.sample_rate:
            self.logger.info(msg, **kwargs)

# Agr√©gation en m√©moire avant √©criture
from collections import defaultdict
import threading

class AggregatingLogger:
    def __init__(self, flush_interval=60):
        self.counts = defaultdict(int)
        self.lock = threading.Lock()

    def count(self, event_type):
        with self.lock:
            self.counts[event_type] += 1

    def flush(self):
        with self.lock:
            for event_type, count in self.counts.items():
                logging.info(f"{event_type}: {count} occurrences")
            self.counts.clear()
```

### 5.2 Donn√©es de session

#### Stockage sobre des sessions

**Ce qui doit √™tre en session :**
- ID utilisateur authentifi√©
- R√¥les/permissions (si pas trop volumineux)
- Pr√©f√©rences essentielles

**Ce qui NE doit PAS √™tre en session :**
- Donn√©es m√©tier compl√®tes (les recharger)
- Historique de navigation complet
- Cache de donn√©es volumineuses
- Donn√©es sensibles non n√©cessaires

```python
# ‚ùå Session volumineuse
session['user'] = {
    'id': 123,
    'profile': {...},  # Donn√©es compl√®tes
    'orders': [...],   # Historique complet
    'preferences': {...},
    'cart': [...],
    'viewed_products': [...],  # Historique navigation
}

# ‚úÖ Session sobre
session['user_id'] = 123
session['roles'] = ['customer']
session['cart_id'] = 'cart_abc123'  # R√©f√©rence, pas les donn√©es
```

#### Expiration des sessions

```python
# Configuration Flask avec expiration
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(hours=2)
app.config['SESSION_REFRESH_EACH_REQUEST'] = True

# Redis avec TTL automatique
@app.before_request
def refresh_session():
    session.modified = True  # Reset le TTL

# Nettoyage des sessions orphelines
def cleanup_orphaned_sessions():
    """Supprime les sessions sans activit√© depuis 24h"""
    cutoff = datetime.utcnow() - timedelta(hours=24)
    redis.delete(*redis.keys('session:*'))  # √Ä adapter
```

### 5.3 Cache et donn√©es temporaires

#### Politique de cache

| Type de cache | TTL recommand√© | Invalidation |
|---------------|----------------|--------------|
| Cache navigateur (assets) | 1 an (versionn√©) | Changement de version |
| Cache CDN | 1h √† 1 jour | Purge manuelle ou TTL |
| Cache applicatif (Redis) | Minutes √† heures | Event-driven |
| Cache base de donn√©es | Secondes √† minutes | Write-through |

**Configuration Redis avec √©viction :**
```
# redis.conf
maxmemory 2gb
maxmemory-policy allkeys-lru  # Supprime les moins r√©cemment utilis√©s

# Ou volatile-ttl pour ne supprimer que les cl√©s avec TTL
maxmemory-policy volatile-ttl
```

#### Nettoyage des donn√©es temporaires

```python
# Nettoyage des fichiers temporaires
import os
import time
from pathlib import Path

def cleanup_temp_files(temp_dir, max_age_hours=24):
    """Supprime les fichiers temporaires de plus de X heures"""
    cutoff = time.time() - (max_age_hours * 3600)

    for file_path in Path(temp_dir).rglob('*'):
        if file_path.is_file():
            if file_path.stat().st_mtime < cutoff:
                file_path.unlink()
                logging.info(f"Deleted temp file: {file_path}")

# Job Kubernetes pour nettoyage r√©gulier
# cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-temp-files
spec:
  schedule: "0 */6 * * *"  # Toutes les 6 heures
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleanup
            image: busybox
            command:
            - find
            - /tmp
            - -type
            - f
            - -mtime
            - "+1"
            - -delete
          restartPolicy: OnFailure
```

### 5.4 Donn√©es de test

#### Isolation des donn√©es de test

```
Environnements strictement s√©par√©s :
‚îú‚îÄ Production : donn√©es r√©elles, jamais de test
‚îú‚îÄ Staging : copie anonymis√©e ou donn√©es synth√©tiques
‚îú‚îÄ Test : donn√©es synth√©tiques ou fixtures
‚îî‚îÄ D√©veloppement : fixtures minimales

‚ùå JAMAIS de donn√©es de production en dev/test
‚ùå JAMAIS de donn√©es de test en production
```

#### G√©n√©ration de donn√©es synth√©tiques

```python
# G√©n√©ration de donn√©es r√©alistes mais fausses
from faker import Faker

fake = Faker('fr_FR')

def generate_test_users(n=100):
    return [
        {
            'id': i,
            'email': fake.email(),
            'name': fake.name(),
            'address': fake.address(),
            'phone': fake.phone_number(),
            'created_at': fake.date_time_this_year()
        }
        for i in range(n)
    ]

def generate_test_orders(users, n=500):
    return [
        {
            'id': i,
            'user_id': fake.random_element(users)['id'],
            'amount': fake.pydecimal(min_value=10, max_value=500, right_digits=2),
            'status': fake.random_element(['pending', 'paid', 'shipped']),
            'created_at': fake.date_time_this_year()
        }
        for i in range(n)
    ]
```

#### Nettoyage post-test

```python
# Fixture avec nettoyage automatique (pytest)
import pytest

@pytest.fixture
def test_data(db):
    """Cr√©e des donn√©es de test et les nettoie apr√®s"""
    # Setup
    users = create_test_users(10)
    orders = create_test_orders(users, 50)

    yield {'users': users, 'orders': orders}

    # Teardown automatique
    db.orders.delete_many({'id': {'$in': [o['id'] for o in orders]}})
    db.users.delete_many({'id': {'$in': [u['id'] for u in users]}})

# CI/CD : nettoyage de l'environnement de test
# .gitlab-ci.yml
cleanup_test_env:
  stage: cleanup
  script:
    - python scripts/cleanup_test_data.py
  only:
    - schedules  # Nettoyage quotidien planifi√©
```

---

## 6. Bases de donn√©es

### 6.1 Optimisation du sch√©ma

#### Normalisation vs d√©normalisation

**Normalisation (3NF) :**
- R√©duit les redondances
- Taille de stockage minimale
- Mais : plus de jointures, potentiellement plus lent

**D√©normalisation contr√¥l√©e :**
- Duplication cibl√©e pour performance
- Acceptable si les donn√©es dupliqu√©es sont stables
- Document√© et maintenu consciemment

```sql
-- Exemple : stocker le nom du client dans la commande
-- √âvite une jointure fr√©quente, mais duplique l'info

-- ‚ùå D√©normalisation excessive
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    customer_id INT,
    customer_name VARCHAR(100),      -- Dupliqu√©
    customer_email VARCHAR(255),     -- Dupliqu√©
    customer_phone VARCHAR(20),      -- Dupliqu√©
    customer_address TEXT,           -- Dupliqu√©
    -- ... et le reste de la commande
);

-- ‚úÖ D√©normalisation raisonnable
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    customer_id INT REFERENCES customers(id),
    customer_name_snapshot VARCHAR(100), -- Seulement ce qui est utile et stable
    -- ... reste de la commande
);
```

#### Choix des types de donn√©es

| Besoin | Type inefficace | Type efficace | √âconomie |
|--------|----------------|---------------|----------|
| Bool√©en | VARCHAR(5) | BOOLEAN | 80% |
| ID interne | BIGINT | INT | 50% |
| UUID | VARCHAR(36) | UUID natif | 60% |
| Status | VARCHAR(20) | ENUM ou SMALLINT | 90% |
| Monnaie | FLOAT | DECIMAL(10,2) | Pr√©cision |
| Timestamp | VARCHAR(30) | TIMESTAMP | 75% |

```sql
-- Exemple d'optimisation
-- ‚ùå Avant
CREATE TABLE events (
    id VARCHAR(36),           -- UUID en texte
    event_type VARCHAR(50),   -- Texte libre
    is_processed VARCHAR(5),  -- "true"/"false"
    created_at VARCHAR(30)    -- "2024-01-15T10:30:00Z"
);

-- ‚úÖ Apr√®s
CREATE TABLE events (
    id UUID DEFAULT gen_random_uuid(),
    event_type SMALLINT,      -- R√©f√©rence vers table de types
    is_processed BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT now()
);

-- √âconomie : ~70% d'espace par ligne
```

### 6.2 Archivage et partitionnement

#### Strat√©gie de partitionnement

**Par date (le plus courant) :**
```sql
-- PostgreSQL : table partitionn√©e par mois
CREATE TABLE events (
    id BIGSERIAL,
    event_type VARCHAR(50),
    payload JSONB,
    created_at TIMESTAMPTZ NOT NULL
) PARTITION BY RANGE (created_at);

-- Partitions par mois
CREATE TABLE events_2024_01 PARTITION OF events
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
CREATE TABLE events_2024_02 PARTITION OF events
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');
-- etc.

-- Cr√©ation automatique des partitions futures (pg_partman)
SELECT partman.create_parent(
    p_parent_table := 'public.events',
    p_control := 'created_at',
    p_type := 'native',
    p_interval := '1 month',
    p_premake := 3  -- Cr√©er 3 mois √† l'avance
);
```

**Archivage vers stockage froid :**
```python
# Script d'archivage des anciennes partitions
import subprocess
from datetime import datetime, timedelta

def archive_old_partition(table_name, partition_date):
    """Archive une partition vers S3 puis la supprime"""

    partition_name = f"{table_name}_{partition_date.strftime('%Y_%m')}"

    # Export vers fichier compress√©
    export_cmd = f"""
        pg_dump -t {partition_name} --data-only --format=custom
        | gzip > /tmp/{partition_name}.dump.gz
    """
    subprocess.run(export_cmd, shell=True, check=True)

    # Upload vers S3 Glacier
    subprocess.run([
        'aws', 's3', 'cp',
        f'/tmp/{partition_name}.dump.gz',
        f's3://archives/database/{partition_name}.dump.gz',
        '--storage-class', 'DEEP_ARCHIVE'
    ], check=True)

    # Supprimer la partition
    subprocess.run([
        'psql', '-c', f'DROP TABLE {partition_name}'
    ], check=True)

# Archiver les partitions de plus de 1 an
cutoff = datetime.now() - timedelta(days=365)
archive_old_partition('events', cutoff)
```

### 6.3 Purge automatis√©e

#### Jobs de purge

```python
# Job de purge avec monitoring
import logging
from datetime import datetime, timedelta
from contextlib import contextmanager

logger = logging.getLogger(__name__)

class DataPurger:
    def __init__(self, db):
        self.db = db
        self.stats = {'deleted': 0, 'errors': 0}

    @contextmanager
    def track_purge(self, table_name):
        start = datetime.now()
        try:
            yield
            duration = (datetime.now() - start).total_seconds()
            logger.info(f"Purge {table_name}: {self.stats['deleted']} rows in {duration:.2f}s")
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"Purge {table_name} failed: {e}")
            raise

    def purge_old_sessions(self, days=7):
        """Supprime les sessions de plus de X jours"""
        with self.track_purge('sessions'):
            cutoff = datetime.utcnow() - timedelta(days=days)
            result = self.db.execute(
                "DELETE FROM sessions WHERE last_activity < %s",
                [cutoff]
            )
            self.stats['deleted'] = result.rowcount

    def purge_old_logs(self, days=90):
        """Supprime les logs de plus de X jours par batch"""
        with self.track_purge('logs'):
            cutoff = datetime.utcnow() - timedelta(days=days)
            total_deleted = 0

            while True:
                # Supprimer par batch de 10000 pour √©viter les locks longs
                result = self.db.execute("""
                    DELETE FROM logs
                    WHERE id IN (
                        SELECT id FROM logs
                        WHERE created_at < %s
                        LIMIT 10000
                    )
                """, [cutoff])

                total_deleted += result.rowcount
                if result.rowcount < 10000:
                    break

            self.stats['deleted'] = total_deleted

    def purge_soft_deleted(self, days=30):
        """Supprime d√©finitivement les enregistrements soft-deleted"""
        with self.track_purge('soft_deleted'):
            cutoff = datetime.utcnow() - timedelta(days=days)

            for table in ['users', 'orders', 'documents']:
                result = self.db.execute(f"""
                    DELETE FROM {table}
                    WHERE deleted_at IS NOT NULL
                    AND deleted_at < %s
                """, [cutoff])
                self.stats['deleted'] += result.rowcount
```

#### Monitoring des purges

```yaml
# Alerting sur les purges (Prometheus)
groups:
  - name: data_purge_alerts
    rules:
      - alert: PurgeJobFailed
        expr: purge_job_last_success_timestamp < (time() - 86400)
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Purge job hasn't run successfully in 24h"

      - alert: DataRetentionExceeded
        expr: data_age_days > data_retention_policy_days
        for: 1h
        labels:
          severity: critical
        annotations:
          summary: "Data older than retention policy exists"
```

### 6.4 Indexation efficace

#### Index utiles vs inutiles

```sql
-- Analyser l'utilisation des index
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan as times_used,
    idx_tup_read,
    idx_tup_fetch,
    pg_size_pretty(pg_relation_size(indexrelid)) as size
FROM pg_stat_user_indexes
ORDER BY idx_scan ASC;

-- Index jamais utilis√©s = candidats √† la suppression
-- Attention : v√©rifier sur une p√©riode repr√©sentative (semaines/mois)
```

**Index √† √©viter :**
```sql
-- ‚ùå Index sur colonnes peu s√©lectives
CREATE INDEX idx_status ON orders(status);  -- Si 90% sont "completed"

-- ‚ùå Index dupliqu√©s
CREATE INDEX idx_user ON orders(user_id);
CREATE INDEX idx_user_date ON orders(user_id, created_at);
-- Le second couvre d√©j√† les requ√™tes du premier

-- ‚ùå Index sur petites tables
CREATE INDEX idx_countries ON countries(code);  -- Table de 200 lignes
```

**Index √† cr√©er :**
```sql
-- ‚úÖ Index composites pour les requ√™tes fr√©quentes
CREATE INDEX idx_orders_user_date ON orders(user_id, created_at DESC);

-- ‚úÖ Index partiels pour les donn√©es actives
CREATE INDEX idx_orders_pending ON orders(created_at)
WHERE status = 'pending';

-- ‚úÖ Index couvrants pour √©viter les acc√®s table
CREATE INDEX idx_orders_summary ON orders(user_id)
INCLUDE (total_amount, status);
```

---

## 7. Stockage fichiers

### 7.1 Organisation et nommage

#### Structure de r√©pertoires

```
storage/
‚îú‚îÄ‚îÄ documents/
‚îÇ   ‚îú‚îÄ‚îÄ contracts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2024/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 02/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2023/
‚îÇ   ‚îî‚îÄ‚îÄ invoices/
‚îÇ       ‚îî‚îÄ‚îÄ {year}/{month}/
‚îú‚îÄ‚îÄ media/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ originals/   # Conserv√©s temporairement
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processed/   # Versions optimis√©es
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ thumbnails/  # Vignettes
‚îÇ   ‚îî‚îÄ‚îÄ videos/
‚îú‚îÄ‚îÄ temp/                 # Purge automatique < 24h
‚îÇ   ‚îî‚îÄ‚îÄ uploads/
‚îî‚îÄ‚îÄ archives/             # Stockage froid
    ‚îî‚îÄ‚îÄ {year}/
```

#### Convention de nommage

```
Format recommand√© : {type}_{id}_{version}_{date}.{ext}

Exemples :
- invoice_12345_v1_20240115.pdf
- contract_abc123_signed_20240110.pdf
- profile_user789_thumb_100x100.jpg

Avantages :
- Tri chronologique naturel
- Identification sans ouvrir
- Versioning explicite
- Unicit√© garantie
```

### 7.2 D√©duplication

#### D√©duplication par hash

```python
import hashlib
from pathlib import Path

class DeduplicatedStorage:
    def __init__(self, base_path):
        self.base_path = Path(base_path)
        self.content_path = self.base_path / 'content'
        self.refs_path = self.base_path / 'refs'

    def store(self, file_content, logical_path):
        """Stocke un fichier avec d√©duplication"""
        # Calculer le hash du contenu
        content_hash = hashlib.sha256(file_content).hexdigest()
        content_file = self.content_path / content_hash[:2] / content_hash

        # Stocker le contenu s'il n'existe pas
        if not content_file.exists():
            content_file.parent.mkdir(parents=True, exist_ok=True)
            content_file.write_bytes(file_content)

        # Cr√©er une r√©f√©rence vers le contenu
        ref_file = self.refs_path / logical_path
        ref_file.parent.mkdir(parents=True, exist_ok=True)
        ref_file.write_text(content_hash)

        return content_hash

    def get(self, logical_path):
        """R√©cup√®re un fichier par son chemin logique"""
        ref_file = self.refs_path / logical_path
        if not ref_file.exists():
            raise FileNotFoundError(logical_path)

        content_hash = ref_file.read_text()
        content_file = self.content_path / content_hash[:2] / content_hash
        return content_file.read_bytes()

    def delete(self, logical_path):
        """Supprime une r√©f√©rence (pas le contenu si encore utilis√©)"""
        ref_file = self.refs_path / logical_path
        if ref_file.exists():
            ref_file.unlink()

    def cleanup_orphans(self):
        """Supprime les contenus sans r√©f√©rence"""
        # Collecter tous les hashs r√©f√©renc√©s
        referenced = set()
        for ref_file in self.refs_path.rglob('*'):
            if ref_file.is_file():
                referenced.add(ref_file.read_text())

        # Supprimer les contenus non r√©f√©renc√©s
        deleted = 0
        for content_file in self.content_path.rglob('*'):
            if content_file.is_file():
                content_hash = content_file.name
                if content_hash not in referenced:
                    content_file.unlink()
                    deleted += 1

        return deleted
```

### 7.3 Compression

#### Compression selon le type

| Type de fichier | D√©j√† compress√© ? | Action recommand√©e |
|-----------------|------------------|-------------------|
| PDF | Souvent oui | V√©rifier, recompresser si possible |
| DOCX/XLSX | Oui (ZIP) | Ne pas recompresser |
| Images (JPEG) | Oui | Optimiser qualit√©/taille |
| Images (PNG) | Oui (lossless) | Convertir en WebP |
| Vid√©os | Oui | Transcoder si n√©cessaire |
| Texte brut | Non | Compresser (gzip) |
| JSON/XML | Non | Compresser (gzip) |
| Archives | Oui | Ne pas recompresser |

**Script d'optimisation d'images :**
```python
from PIL import Image
from pathlib import Path

def optimize_image(input_path, output_path, max_size=(1920, 1080), quality=85):
    """Optimise une image : redimensionne et compresse"""
    img = Image.open(input_path)

    # Redimensionner si n√©cessaire
    if img.width > max_size[0] or img.height > max_size[1]:
        img.thumbnail(max_size, Image.LANCZOS)

    # Convertir en RGB si n√©cessaire (pour JPEG)
    if img.mode in ('RGBA', 'P'):
        img = img.convert('RGB')

    # Sauvegarder avec compression
    img.save(output_path, 'JPEG', quality=quality, optimize=True)

    # Stats
    original_size = Path(input_path).stat().st_size
    new_size = Path(output_path).stat().st_size
    reduction = (1 - new_size / original_size) * 100

    return {'original': original_size, 'new': new_size, 'reduction': f'{reduction:.1f}%'}
```

### 7.4 Tiering et archivage

#### Classes de stockage cloud

**AWS S3 :**
```python
import boto3
from datetime import datetime, timedelta

s3 = boto3.client('s3')

# Politique de lifecycle
lifecycle_config = {
    'Rules': [
        {
            'ID': 'ArchiveOldFiles',
            'Status': 'Enabled',
            'Filter': {'Prefix': 'documents/'},
            'Transitions': [
                {
                    'Days': 30,
                    'StorageClass': 'STANDARD_IA'  # Infrequent Access
                },
                {
                    'Days': 90,
                    'StorageClass': 'GLACIER_IR'   # Glacier Instant Retrieval
                },
                {
                    'Days': 365,
                    'StorageClass': 'DEEP_ARCHIVE' # Archive long terme
                }
            ],
            'Expiration': {
                'Days': 2555  # Suppression apr√®s 7 ans
            }
        }
    ]
}

s3.put_bucket_lifecycle_configuration(
    Bucket='my-bucket',
    LifecycleConfiguration=lifecycle_config
)
```

**Co√ªts par classe (indicatif) :**
| Classe | Stockage/Go/mois | R√©cup√©ration |
|--------|-----------------|--------------|
| Standard | $0.023 | Imm√©diat |
| Standard-IA | $0.0125 | Imm√©diat, frais acc√®s |
| Glacier Instant | $0.004 | Millisecondes, frais acc√®s |
| Glacier Flexible | $0.0036 | Minutes √† heures |
| Deep Archive | $0.00099 | 12-48 heures |

---

## 8. Analytics et Business Intelligence

### 8.1 Collecte raisonn√©e

#### D√©finir les m√©triques essentielles

**Framework AARRR (Pirate Metrics) :**
```
Acquisition  : D'o√π viennent les utilisateurs ?
Activation   : Ont-ils une bonne premi√®re exp√©rience ?
Retention    : Reviennent-ils ?
Referral     : Recommandent-ils ?
Revenue      : G√©n√®rent-ils du revenu ?

Pour chaque √©tape : 2-3 m√©triques cl√©s maximum
Pas de "nice to have" qui ne sera jamais analys√©
```

**Exemple de m√©triques essentielles vs excessives :**
```
‚ùå Collecte excessive :
- Toutes les pages vues avec d√©tail
- Tous les clics avec coordonn√©es
- Temps pass√© par √©l√©ment
- Mouvements de souris
- Historique complet par utilisateur

‚úÖ Collecte sobre :
- Pages vues (agr√©g√©es par page)
- Conversions (√©v√©nements cl√©s)
- Origine du trafic (agr√©g√©e)
- Temps moyen par session
```

### 8.2 Agr√©gation vs donn√©es brutes

#### Strat√©gie d'agr√©gation

```python
# Pipeline d'agr√©gation progressive
class AnalyticsPipeline:
    def __init__(self):
        self.raw_retention = timedelta(days=7)
        self.hourly_retention = timedelta(days=30)
        self.daily_retention = timedelta(days=365)
        self.monthly_retention = timedelta(days=365*5)

    def ingest_event(self, event):
        """Ing√®re un √©v√©nement brut"""
        # Stockage temporaire des √©v√©nements bruts
        self.raw_store.store(event)

        # Mise √† jour des agr√©gats en temps r√©el
        self.update_realtime_counters(event)

    def hourly_aggregation(self):
        """Job horaire : agr√®ge les donn√©es brutes"""
        hour = datetime.utcnow().replace(minute=0, second=0, microsecond=0)
        events = self.raw_store.get_events(hour - timedelta(hours=1), hour)

        aggregates = {
            'timestamp': hour,
            'page_views': defaultdict(int),
            'unique_visitors': set(),
            'conversions': defaultdict(int),
        }

        for event in events:
            if event['type'] == 'page_view':
                aggregates['page_views'][event['page']] += 1
            aggregates['unique_visitors'].add(event['session_id'])
            if event.get('conversion'):
                aggregates['conversions'][event['conversion_type']] += 1

        # Transformer les sets en counts
        aggregates['unique_visitors'] = len(aggregates['unique_visitors'])

        self.hourly_store.store(aggregates)

    def daily_aggregation(self):
        """Job quotidien : agr√®ge les donn√©es horaires"""
        date = datetime.utcnow().date() - timedelta(days=1)
        hourly_data = self.hourly_store.get_day(date)

        daily_aggregate = {
            'date': date,
            'total_page_views': sum(h['page_views'].values() for h in hourly_data),
            'unique_visitors': ...,  # Recalculer depuis les raw ou estimer
            'conversions': ...,
        }

        self.daily_store.store(daily_aggregate)

    def cleanup_old_data(self):
        """Supprime les donn√©es au-del√† de leur r√©tention"""
        now = datetime.utcnow()
        self.raw_store.delete_before(now - self.raw_retention)
        self.hourly_store.delete_before(now - self.hourly_retention)
        self.daily_store.delete_before(now - self.daily_retention)
```

### 8.3 Alternatives sobres aux analytics classiques

#### Analytics sans tracking individuel

**Plausible Analytics :**
- Open source, auto-h√©bergeable
- Pas de cookies
- Donn√©es agr√©g√©es uniquement
- Script < 1KB

**Umami :**
- Open source
- Privacy-first
- Pas de donn√©es personnelles

**Simple Analytics :**
- Aucune donn√©e personnelle
- Conforme RGPD par d√©faut

#### Analytics c√¥t√© serveur

```python
# Analytics minimaliste c√¥t√© serveur
from collections import defaultdict
from datetime import datetime

class ServerSideAnalytics:
    def __init__(self):
        self.daily_stats = defaultdict(lambda: defaultdict(int))

    def track_page_view(self, path, user_agent):
        """Track une page vue sans cookies ni JavaScript"""
        today = datetime.utcnow().date()
        self.daily_stats[today][path] += 1

        # Cat√©goriser le device sans fingerprinting
        if 'Mobile' in user_agent:
            self.daily_stats[today]['mobile_views'] += 1
        else:
            self.daily_stats[today]['desktop_views'] += 1

    def get_stats(self, date):
        return dict(self.daily_stats[date])

# Middleware Flask
@app.after_request
def track_analytics(response):
    if response.status_code == 200:
        analytics.track_page_view(
            request.path,
            request.headers.get('User-Agent', '')
        )
    return response
```

### 8.4 R√©tention des donn√©es BI

#### Politique de r√©tention diff√©renci√©e

```
Donn√©es brutes (events) :
‚îú‚îÄ Temps r√©el ‚Üí 24h (pour debug)
‚îú‚îÄ D√©tail ‚Üí 7 jours
‚îî‚îÄ Puis supprim√©es

Agr√©gats horaires :
‚îú‚îÄ D√©tail par page/source ‚Üí 30 jours
‚îî‚îÄ Puis agr√©g√©s en quotidien

Agr√©gats quotidiens :
‚îú‚îÄ D√©tail ‚Üí 1 an
‚îî‚îÄ Puis agr√©g√©s en mensuel

Agr√©gats mensuels :
‚îî‚îÄ Conserv√©s 5 ans
```

---

## 9. Emails et collaboration

### 9.1 Politiques de messagerie

#### Quotas et limites

| √âl√©ment | Limite recommand√©e | Justification |
|---------|-------------------|---------------|
| Taille bo√Æte | 2-5 Go | Encourage le tri |
| Taille pi√®ce jointe | 10-25 Mo | Forcer partage de liens |
| R√©tention corbeille | 30 jours | R√©cup√©ration possible |
| R√©tention spam | 30 jours | Faux positifs |
| Archivage auto | 2 ans ‚Üí archive | L√©g√®ret√© bo√Æte active |

#### Configuration Exchange/M365

```powershell
# D√©finir les quotas par d√©faut
Set-Mailbox -Identity "Database01" `
    -IssueWarningQuota 4GB `
    -ProhibitSendQuota 4.5GB `
    -ProhibitSendReceiveQuota 5GB

# Politique de r√©tention
New-RetentionPolicy -Name "Standard Retention" `
    -RetentionPolicyTagLinks @(
        "Delete after 2 years",
        "Move to archive after 1 year",
        "Delete Junk Email after 30 days",
        "Delete Deleted Items after 30 days"
    )

# Appliquer la politique
Set-Mailbox -Identity "user@domain.com" `
    -RetentionPolicy "Standard Retention"
```

### 9.2 Pi√®ces jointes

#### Strat√©gie "liens plut√¥t que fichiers"

**Mauvaise pratique :**
```
Email avec pi√®ce jointe de 15 Mo
‚îú‚îÄ Envoy√© √† 20 personnes
‚îú‚îÄ Stock√© dans 20 bo√Ætes √ó taille
‚îú‚îÄ Sauvegard√© quotidiennement
‚îî‚îÄ Total : 15 Mo √ó 20 √ó 30 jours √ó 2 (backup) = 18 Go
```

**Bonne pratique :**
```
Email avec lien vers fichier partag√©
‚îú‚îÄ Fichier stock√© 1 fois (15 Mo)
‚îú‚îÄ Lien ne prend quasi rien
‚îú‚îÄ Versioning g√©r√© par la plateforme
‚îî‚îÄ Total : 15 Mo √ó 3 versions = 45 Mo
```

**Impl√©mentation technique :**
```python
# Intercepteur de pi√®ces jointes (Exchange Transport Rule simul√©)
def process_outgoing_email(email):
    attachments = email.attachments
    new_body = email.body

    for attachment in attachments:
        if attachment.size > 5_000_000:  # > 5 Mo
            # Upload vers SharePoint/OneDrive
            link = upload_to_sharepoint(attachment)

            # Remplacer par un lien dans le corps
            new_body += f"\n\nüìé {attachment.name}: {link}"

            # Retirer la pi√®ce jointe
            email.remove_attachment(attachment)

    email.body = new_body
    return email
```

### 9.3 Stockage collaboratif

#### Gouvernance SharePoint/OneDrive

**Structure recommand√©e :**
```
SharePoint :
‚îú‚îÄ Sites d'√©quipe (contenus collaboratifs)
‚îÇ   ‚îú‚îÄ Projets actifs
‚îÇ   ‚îî‚îÄ Documentation √©quipe
‚îú‚îÄ Sites de communication (contenus publi√©s)
‚îÇ   ‚îî‚îÄ Intranet, news
‚îî‚îÄ Archives (automatique apr√®s 2 ans d'inactivit√©)

OneDrive :
‚îú‚îÄ Travail personnel en cours
‚îú‚îÄ Brouillons avant partage
‚îî‚îÄ PAS de sauvegarde de fichiers partag√©s
```

**Politiques de r√©tention M365 :**
```powershell
# Cr√©er une politique de r√©tention
New-RetentionPolicy -Name "Document Retention" `
    -Mode Enforce `
    -SharePointLocation All `
    -OneDriveLocation All `
    -RetentionDuration Days730 `  # 2 ans
    -RetentionAction Delete
```

### 9.4 Messagerie instantan√©e

#### R√©tention Slack/Teams

**Slack :**
```
Policies recommand√©es :
‚îú‚îÄ Messages DM : 90 jours
‚îú‚îÄ Channels publics : 1 an
‚îú‚îÄ Channels priv√©s : 6 mois
‚îî‚îÄ Fichiers : 90 jours (forcer liens externes)
```

**Microsoft Teams :**
```powershell
# Politique de r√©tention Teams
New-TeamsRetentionPolicy -Name "Standard" `
    -ChatRetentionDuration 90 `  # Chats priv√©s : 90 jours
    -ChannelRetentionDuration 365  # Canaux : 1 an
```

#### Nettoyage des fichiers partag√©s

```python
# Script de nettoyage des fichiers Slack anciens
from slack_sdk import WebClient

client = WebClient(token="xoxb-...")

def cleanup_old_files(days=90):
    """Supprime les fichiers de plus de X jours"""
    from datetime import datetime, timedelta

    cutoff = datetime.now() - timedelta(days=days)
    cutoff_ts = cutoff.timestamp()

    # Lister les fichiers
    response = client.files_list(ts_to=cutoff_ts)

    for file in response['files']:
        # Supprimer le fichier
        client.files_delete(file=file['id'])
        print(f"Deleted: {file['name']} ({file['size']} bytes)")
```

---

## 10. Mise en ≈ìuvre

### 10.1 Plan d'action type

#### Phase 1 : Diagnostic (1-2 mois)

```
Semaine 1-2 : Inventaire
‚îú‚îÄ Lister tous les syst√®mes de stockage
‚îú‚îÄ Collecter les volum√©tries
‚îú‚îÄ Identifier les responsables

Semaine 3-4 : Analyse
‚îú‚îÄ Cartographier les flux de donn√©es
‚îú‚îÄ Identifier le dark data (fichiers non acc√©d√©s)
‚îú‚îÄ Estimer les co√ªts et l'empreinte

Semaine 5-6 : Diagnostic
‚îú‚îÄ Identifier les quick wins
‚îú‚îÄ Prioriser les actions
‚îú‚îÄ Estimer les gains potentiels

Semaine 7-8 : Restitution
‚îú‚îÄ Pr√©sentation des r√©sultats
‚îú‚îÄ Validation du plan d'action
‚îî‚îÄ D√©finition des objectifs
```

#### Phase 2 : Quick wins (2-3 mois)

```
Actions √† fort impact, faible effort :

1. Purge du dark data √©vident
   ‚îú‚îÄ Fichiers temporaires oubli√©s
   ‚îú‚îÄ Comptes utilisateurs partis
   ‚îî‚îÄ Donn√©es de test en production

2. Configuration des r√©tentions
   ‚îú‚îÄ Emails (corbeille, spam)
   ‚îú‚îÄ Logs (niveaux, dur√©es)
   ‚îî‚îÄ Backups (consolidation)

3. Sensibilisation utilisateurs
   ‚îú‚îÄ Communication sur les enjeux
   ‚îú‚îÄ Outils de nettoyage individuel
   ‚îî‚îÄ Premiers challenges de r√©duction
```

#### Phase 3 : Structuration (3-6 mois)

```
Actions structurantes :

1. Politique de r√©tention
   ‚îú‚îÄ R√©daction et validation
   ‚îú‚îÄ D√©ploiement technique
   ‚îî‚îÄ Formation des √©quipes

2. Gouvernance
   ‚îú‚îÄ Nomination des Data Owners
   ‚îú‚îÄ Processus de validation
   ‚îî‚îÄ Revues p√©riodiques

3. Automatisation
   ‚îú‚îÄ Jobs de purge automatiques
   ‚îú‚îÄ Alerting et monitoring
   ‚îî‚îÄ Rapports de suivi
```

#### Phase 4 : Am√©lioration continue

```
Actions p√©rennes :

1. Monitoring continu
   ‚îú‚îÄ Dashboard de suivi
   ‚îú‚îÄ Alertes de d√©passement
   ‚îî‚îÄ Revues mensuelles

2. Optimisation progressive
   ‚îú‚îÄ D√©duplication
   ‚îú‚îÄ Compression
   ‚îî‚îÄ Tiering

3. Culture de la sobri√©t√©
   ‚îú‚îÄ Int√©gration dans les projets
   ‚îú‚îÄ Formation continue
   ‚îî‚îÄ Objectifs individuels et collectifs
```

### 10.2 Outils et automatisation

#### Outils d'analyse

**Analyse de stockage fichiers :**
- **TreeSize** (Windows) : visualisation arborescente
- **ncdu** (Linux) : analyse en ligne de commande
- **WinDirStat** : visualisation graphique
- **Disk Inventory X** (Mac) : analyse visuelle

**Analyse cloud :**
- **AWS S3 Analytics** : patterns d'acc√®s
- **Azure Storage Analytics** : m√©triques d√©taill√©es
- **GCS Insights** : recommandations

**Analyse bases de donn√©es :**
```sql
-- PostgreSQL : taille des tables
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname || '.' || tablename)) as total_size,
    pg_size_pretty(pg_relation_size(schemaname || '.' || tablename)) as data_size,
    pg_size_pretty(pg_indexes_size(schemaname || '.' || tablename)) as index_size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname || '.' || tablename) DESC
LIMIT 20;
```

#### Framework d'automatisation

```python
# Framework de gestion de la r√©tention
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
import logging

class RetentionAction(Enum):
    KEEP = "keep"
    ARCHIVE = "archive"
    DELETE = "delete"

@dataclass
class RetentionRule:
    name: str
    data_type: str
    retention_days: int
    action_after: RetentionAction
    archive_location: str = None

class RetentionManager:
    def __init__(self):
        self.rules = []
        self.logger = logging.getLogger(__name__)

    def add_rule(self, rule: RetentionRule):
        self.rules.append(rule)

    def evaluate(self, data_item):
        """√âvalue quelle action appliquer √† un item"""
        for rule in self.rules:
            if self._matches(data_item, rule):
                age_days = (datetime.now() - data_item.created_at).days
                if age_days > rule.retention_days:
                    return rule.action_after, rule
        return RetentionAction.KEEP, None

    def apply_rules(self, data_source, dry_run=True):
        """Applique les r√®gles de r√©tention √† une source de donn√©es"""
        stats = {'archived': 0, 'deleted': 0, 'kept': 0}

        for item in data_source.iterate():
            action, rule = self.evaluate(item)

            if action == RetentionAction.DELETE:
                if not dry_run:
                    data_source.delete(item)
                stats['deleted'] += 1
                self.logger.info(f"{'Would delete' if dry_run else 'Deleted'}: {item.id}")

            elif action == RetentionAction.ARCHIVE:
                if not dry_run:
                    data_source.archive(item, rule.archive_location)
                stats['archived'] += 1

            else:
                stats['kept'] += 1

        return stats

# Usage
manager = RetentionManager()
manager.add_rule(RetentionRule(
    name="Old logs",
    data_type="log",
    retention_days=90,
    action_after=RetentionAction.DELETE
))
manager.add_rule(RetentionRule(
    name="Archive old documents",
    data_type="document",
    retention_days=730,
    action_after=RetentionAction.ARCHIVE,
    archive_location="s3://archives/documents/"
))

# Dry run d'abord
stats = manager.apply_rules(log_database, dry_run=True)
print(f"Dry run results: {stats}")

# Puis ex√©cution r√©elle
stats = manager.apply_rules(log_database, dry_run=False)
```

### 10.3 Indicateurs de suivi

#### KPIs essentiels

| Indicateur | D√©finition | Cible |
|------------|------------|-------|
| Volume total | Stockage total utilis√© | Stabilisation ou r√©duction |
| Taux de croissance | Croissance mensuelle | < 5%/mois |
| Dark data ratio | % donn√©es non acc√©d√©es > 1 an | < 20% |
| Co√ªt par To | Co√ªt moyen de stockage | R√©duction YoY |
| Taux de purge | Donn√©es purg√©es / √©ligibles | > 95% |
| D√©lai traitement RGPD | Temps moyen de r√©ponse | < 15 jours |

#### Dashboard de suivi

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Dashboard Sobri√©t√© des Donn√©es                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  VOLUME TOTAL                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ  120 To (objectif: 100 To)              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                         -15% vs objectif            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  R√âPARTITION PAR TYPE                                       ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Documents : 45 To (38%)              ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Emails : 30 To (25%)                 ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Bases de donn√©es : 25 To (21%)       ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Logs : 15 To (12%)                   ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Autres : 5 To (4%)                   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  DARK DATA                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 18% des donn√©es non acc√©d√©es depuis > 1 an          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ = 21.6 To = 5 200‚Ç¨/mois de stockage                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Objectif : < 10%                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  PURGES CE MOIS                                             ‚îÇ
‚îÇ  ‚îú‚îÄ Logs expir√©s : 2.3 To ‚úì                                ‚îÇ
‚îÇ  ‚îú‚îÄ Sessions obsol√®tes : 150 Go ‚úì                          ‚îÇ
‚îÇ  ‚îú‚îÄ Fichiers temporaires : 80 Go ‚úì                         ‚îÇ
‚îÇ  ‚îî‚îÄ √âchecs de purge : 2 (voir alertes)                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  EMPREINTE CARBONE ESTIM√âE                                  ‚îÇ
‚îÇ  84 kgCO2e/an (-12% vs ann√©e pr√©c√©dente)                   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 10.4 Sensibilisation utilisateurs

#### Programme de sensibilisation

**Module 1 : Les enjeux (30 min)**
- Impact environnemental du stockage
- Co√ªts cach√©s de l'accumulation
- Risques de s√©curit√© du dark data
- Obligations l√©gales (RGPD)

**Module 2 : Bonnes pratiques (1h)**
- Tri des emails et pi√®ces jointes
- Organisation des fichiers
- Alternatives aux copies (liens)
- Outils de nettoyage

**Module 3 : Actions concr√®tes (exercices)**
- Audit de son espace personnel
- Challenge de r√©duction (1 Go lib√©r√©)
- Mise en place de routines de tri

#### Gamification

```
Challenge Sobri√©t√© Donn√©es - Janvier 2025

üèÜ Classement par √©quipe :
1. ü•á Marketing : 45 Go lib√©r√©s
2. ü•à IT : 38 Go lib√©r√©s
3. ü•â RH : 25 Go lib√©r√©s

üë§ Top contributeurs :
1. Marie D. : 8.5 Go
2. Thomas L. : 6.2 Go
3. Sophie M. : 5.8 Go

üéØ Objectif collectif : 200 Go
üìä Progression : 108 Go (54%)

üí° Astuce du jour :
"Remplacez les pi√®ces jointes par des liens
vers des fichiers partag√©s !"
```

---

## Checklist sobri√©t√© des donn√©es

### Gouvernance

- [ ] Politique de r√©tention document√©e et valid√©e
- [ ] Data Owners nomm√©s pour chaque domaine
- [ ] Processus de revue p√©riodique en place
- [ ] Formation des √©quipes r√©alis√©e

### Technique

- [ ] Inventaire des syst√®mes de stockage complet
- [ ] Jobs de purge automatiques configur√©s
- [ ] Monitoring et alerting en place
- [ ] Backups optimis√©s (pas de redondance excessive)

### Donn√©es personnelles

- [ ] Dur√©es de conservation RGPD d√©finies
- [ ] Processus d'effacement op√©rationnel
- [ ] Minimisation appliqu√©e aux collectes
- [ ] Anonymisation en place pour les analytics

### Utilisateurs

- [ ] Sensibilisation effectu√©e
- [ ] Outils de nettoyage fournis
- [ ] Quotas et limites configur√©s
- [ ] Rapports de consommation accessibles

### Suivi

- [ ] Dashboard de suivi op√©rationnel
- [ ] KPIs d√©finis et mesur√©s
- [ ] Objectifs de r√©duction fix√©s
- [ ] Revue mensuelle planifi√©e

---

## Ressources compl√©mentaires

### R√©glementation

- **RGPD** : texte officiel et guidelines CNIL
- **Guide CNIL** : dur√©es de conservation
- **ANSSI** : recommandations de s√©curit√© des donn√©es

### Outils

- **Data governance** : Collibra, Alation, Apache Atlas
- **Analyse stockage** : TreeSize, ncdu, S3 Analytics
- **Purge automatis√©e** : custom scripts, AWS Lifecycle, Azure Retention

### Formations

- **MOOC CNIL** : atelier RGPD
- **Certification DPO** : formations professionnelles
- **Green IT** : MOOC INR, formations GreenIT.fr

---

La sobri√©t√© des donn√©es, c'est refuser l'accumulation par d√©faut. Chaque donn√©e conserv√©e doit avoir une raison d'√™tre, une dur√©e de vie d√©finie, et un responsable qui pourra d√©cider de sa suppression. C'est un changement de paradigme : passer de "on garde tout au cas o√π" √† "on ne conserve que ce qui est n√©cessaire".
